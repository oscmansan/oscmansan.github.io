---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## About me

Hi, my name is Oscar Mañas.

I am a PhD student at [Mila](https://mila.quebec/en/) and [Université de Montréal](https://diro.umontreal.ca/english/home/), advised by Prof. [Aishwarya Agrawal](https://www.iro.umontreal.ca/~agrawal/). I am also a research scientist intern at [Meta AI](https://ai.meta.com/research/), advised by Dr. [Michal Drozdzal](https://www.linkedin.com/in/michal-drozdzal-a36b9b42) and Prof. [Adriana Romero](https://sites.google.com/site/adriromsor/home).

My research interests lie at the intersection of computer vision and natural language processing. Just like humans (and other animals), I believe machines should have a holistic understanding of the world around them. This means working with multiple sensory modalities. Vision and language are two particularly interesting modalities. On one hand, they are complementary: vision is a low-level perceptual modality while language is an abstract human construct. On the other hand, they are believed to be two essential modalities for solving [AI-complete problems](https://en.wikipedia.org/wiki/AI-complete).

I am generally interested in multimodal vision-language generative models, i.e. models capable of generating images and/or text conditioned on multimodal inputs. Generating new content requires learning and composing patterns from existing data, i.e. modeling the underlying data distribution. When this data represents the real world, generative models become effective “world models”. This idea has numerous applications. For example, text-conditioned image generation models can synthesize data on demand for training recognition/representation learning models on new tasks/skills. Furthermore, given the semantic and compositional nature of language, (large) language models can serve as reasoning engines. By aligning language models with vision encoders, we can build powerful multimodal systems capable of both perceiving and reasoning, which can be deployed as multimodal assistants (e.g. to aid visually-impaired users).

Previously, I was a research intern at [Element AI](https://www.elementai.com/research) in Montreal, advised by Dr. [Pau Rodríguez](https://prlz77.github.io) and Dr. [David Vázquez](http://www.david-vazquez.com). I obtained a [M.Sc. in Computer Vision](https://pagines.uab.cat/mcv) from [Universitat Autònoma de Barcelona](https://www.uab.cat), and I carried out my master's thesis at the [Image Processing Group](https://imatge.upc.edu/web/) advised by Prof. [Xavier Giró](https://imatge.upc.edu/web/people/xavier-giro). Before, I obtained a [B.Sc. in Computer Science](https://www.fib.upc.edu/en/studies/bachelors-degrees/bachelor-degree-informatics-engineering) from [Universitat Politècnica de Catalunya](https://www.upc.edu), and I carried out my bachelor's thesis at the [Architectures and Compilers Group](https://arco.e.ac.upc.edu/wiki/index.php/Main_Page) advised by Prof. [Antonio Gonzalez](https://people.ac.upc.edu/antonio) and Dr. [Jose-Maria Arnau](http://jarnau.site.ac.upc.edu).
